{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RISC-V KernelCI bridge FastAPI app that provides REST services which give any maintainer or kernel developer a simple way of submitting tests to KernelCI without adjusting any existing infrastructure to KernelCI's needs. The project makes use of KCIDB to submit results, meaning you'll need a token to perform submissions. To obtain a token please follow the instructions described in KCIDB github repository . Project layout docker-compose-dev.yml # Docker compose file to use in development docker-compose.yml # Docker compose file to use in production Makefile # Used to run typically used commands log.ini # Logging configuration file caddy/ Caddyfile # Caddy configuration file backend/ Dockerfile # FastAPI app Dockerfile app/ app/ # FastAPI app main directory api/ # Main API directory v1/ endpoints/ # Contains all endpoint implementations builds.py # Submit a build endpoint tests.py # Submit tests endpoint boot.py # Submit boot tests endpoint ... api.py # Gathers all endpoints into a single router core/ config.py # Contains main settings class db.py # Handles DB specific settings runners.py # Contains all general runner functions middleware/ # All middlewares go here json_middleware.py # Specific for wrongly formed json sent by TuxSuite models/ # Database models (use SQLModel/SQLAlchemy) builds.py tests.py schemas/ # Pydantic schemas used when parsing requests bodies builds.py # Models for the submit build service tests.py # Models for the submit tests service ... services/ # Contains code that connects to third-party services (runners may be added here) kcidb_services.py # KCIDB related functions runner_service.py # General runner functions ... utils/ # Utilitary functions and classes exceptions.py test_parser.py main.py # Defines the main FastAPI app pypoetry.toml poetry.lock Sytem design Below there is a diagram portraing the design of the system: Available services The app provides services that allow running kernel tests and builds, and submit tests. There is a separate sync service which is meant to be called once a day to retry failed submissions. Run tests The endpoint is /api/v1/tests/run . To run tests you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"build_id\": \"string\", \"kernel_image_url\": \"string\", \"modules_url\": \"string\", \"tests\": [ \"string\" ], \"collection\": \"string\" } The specific details on each of the fields can be found in the TestSuite schema . Run boot tests The endpoint is /api/v1/boot-test/run . To run boot tests you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"build_id\": \"string\", \"kernel_image_url\": \"string\", \"modules_url\": \"string\" } The specific details on each of the fields can be found in the TestSuite schema . Run builds The endpoint is /api/v1/builds/run . To run builds you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"toolchain\": \"string\", \"tree\": \"string\", \"branch\": \"string\", \"kconfig\": \"string\", \"fragments\": [ \"string\" ] } The specific details on each of the fields can be found in the BuildData schema . Submit tests Tests can be run in an outside system but still use this app to submit results to KernelCI. To do so you have to send a POST HTTP to /api/v1/tests/submit request with a body with this structure: { \"test_uid\": \"string\", \"build_id\": \"string\", \"tests\": [ { \"test_name\": \"string\", \"test_collection\": \"string\", \"result\": \"string\", \"logs\": \"string\" } ] } The specific details on each of the fields can be found in the RunnerTestsResults schema . Synchronize submissions A POST HTTP to /api/v1/sync/results synchronizes non-submitted tests results. While /api/v1/sync/builds synchronizes non-submitted builds. Both of these services expect and empty body. This endpoints correspond to the RISC-V Sync Service seen in the system diagram.","title":"Home"},{"location":"#risc-v-kernelci-bridge","text":"FastAPI app that provides REST services which give any maintainer or kernel developer a simple way of submitting tests to KernelCI without adjusting any existing infrastructure to KernelCI's needs. The project makes use of KCIDB to submit results, meaning you'll need a token to perform submissions. To obtain a token please follow the instructions described in KCIDB github repository .","title":"RISC-V KernelCI bridge"},{"location":"#project-layout","text":"docker-compose-dev.yml # Docker compose file to use in development docker-compose.yml # Docker compose file to use in production Makefile # Used to run typically used commands log.ini # Logging configuration file caddy/ Caddyfile # Caddy configuration file backend/ Dockerfile # FastAPI app Dockerfile app/ app/ # FastAPI app main directory api/ # Main API directory v1/ endpoints/ # Contains all endpoint implementations builds.py # Submit a build endpoint tests.py # Submit tests endpoint boot.py # Submit boot tests endpoint ... api.py # Gathers all endpoints into a single router core/ config.py # Contains main settings class db.py # Handles DB specific settings runners.py # Contains all general runner functions middleware/ # All middlewares go here json_middleware.py # Specific for wrongly formed json sent by TuxSuite models/ # Database models (use SQLModel/SQLAlchemy) builds.py tests.py schemas/ # Pydantic schemas used when parsing requests bodies builds.py # Models for the submit build service tests.py # Models for the submit tests service ... services/ # Contains code that connects to third-party services (runners may be added here) kcidb_services.py # KCIDB related functions runner_service.py # General runner functions ... utils/ # Utilitary functions and classes exceptions.py test_parser.py main.py # Defines the main FastAPI app pypoetry.toml poetry.lock","title":"Project layout"},{"location":"#sytem-design","text":"Below there is a diagram portraing the design of the system:","title":"Sytem design"},{"location":"#available-services","text":"The app provides services that allow running kernel tests and builds, and submit tests. There is a separate sync service which is meant to be called once a day to retry failed submissions.","title":"Available services"},{"location":"#run-tests","text":"The endpoint is /api/v1/tests/run . To run tests you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"build_id\": \"string\", \"kernel_image_url\": \"string\", \"modules_url\": \"string\", \"tests\": [ \"string\" ], \"collection\": \"string\" } The specific details on each of the fields can be found in the TestSuite schema .","title":"Run tests"},{"location":"#run-boot-tests","text":"The endpoint is /api/v1/boot-test/run . To run boot tests you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"build_id\": \"string\", \"kernel_image_url\": \"string\", \"modules_url\": \"string\" } The specific details on each of the fields can be found in the TestSuite schema .","title":"Run boot tests"},{"location":"#run-builds","text":"The endpoint is /api/v1/builds/run . To run builds you need to send a POST HTTP request with a body with this structure: { \"runner\": \"string\", \"toolchain\": \"string\", \"tree\": \"string\", \"branch\": \"string\", \"kconfig\": \"string\", \"fragments\": [ \"string\" ] } The specific details on each of the fields can be found in the BuildData schema .","title":"Run builds"},{"location":"#submit-tests","text":"Tests can be run in an outside system but still use this app to submit results to KernelCI. To do so you have to send a POST HTTP to /api/v1/tests/submit request with a body with this structure: { \"test_uid\": \"string\", \"build_id\": \"string\", \"tests\": [ { \"test_name\": \"string\", \"test_collection\": \"string\", \"result\": \"string\", \"logs\": \"string\" } ] } The specific details on each of the fields can be found in the RunnerTestsResults schema .","title":"Submit tests"},{"location":"#synchronize-submissions","text":"A POST HTTP to /api/v1/sync/results synchronizes non-submitted tests results. While /api/v1/sync/builds synchronizes non-submitted builds. Both of these services expect and empty body. This endpoints correspond to the RISC-V Sync Service seen in the system diagram.","title":"Synchronize submissions"},{"location":"api/","text":"API Reference Tests Services to handle tests runs and submissions. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import tests from fastapi import FastAPI app = FastAPI() app.include_router(tests.router) run_tests(tests_data, session, request) async Schedules tests in a runner. We only scheduled tests that have not been run for before for the build. Parameters: tests_data ( TestSuite ) \u2013 Tests to schedule session ( SessionDep ) \u2013 Database session request ( Request ) \u2013 Request object, allows us to obtain a URL from a function name Source code in app/api/v1/endpoints/tests.py @router.post(\"/run\", status_code=204) async def run_tests(tests_data: TestSuite, session: SessionDep, request: Request): \"\"\" Schedules tests in a runner. We only scheduled tests that have not been run for before for the build. :param tests_data: Tests to schedule :param session: Database session :param request: Request object, allows us to obtain a URL from a function name \"\"\" tests_to_run = [] for test in tests_data.tests: tests_has_been_run = session.exec(select(func.count(RunTest.build_id)) .where(RunTest.build_id == tests_data.build_id) .where(RunTest.test == test)).one() if tests_has_been_run > 0: logging.info(f\"Test {test} from {tests_data.collection} has already been run for {tests_data.build_id}\") continue tests_to_run.append(test) if len(tests_to_run) == 0: logging.info(f\"No tests to run for build: {tests_data.build_id}\") return tests_runner = get_test_runner(tests_data.runner) test_uid = tests_runner(tests_data.kernel_image_url, tests_data.modules_url, tests_to_run, \"qemu-riscv64\", str(request.url_for(get_test_callback_funcname(tests_data.runner)))) scheduled_test = ScheduledTest(test_uid=test_uid, build_id=tests_data.build_id, test_collection=tests_data.collection, tests=tests_to_run, runner=tests_data.runner) session.add(scheduled_test) for test in tests_to_run: run_test = RunTest(build_id=tests_data.build_id, test=test) session.add(run_test) # Both queries are run in a transaction session.commit() submit_results(results, session) async Submit test results to KCIDB. In this case test runner is not relevant. Parameters: results ( RunnerTestsResults ) \u2013 Test results passed as the body of the request session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tests.py @router.post(\"/submit\", status_code=204) async def submit_results(results: RunnerTestsResults, session: SessionDep): \"\"\" Submit test results to KCIDB. In this case test runner is not relevant. :param results: Test results passed as the body of the request :param session: Database session \"\"\" logging.info(f\"Received results for {results.test_uid}\") parsed_results = parse_results2kcidb(results) json_results = [item.to_json() for item in parsed_results] try: submit_kcidb(json_results) except KCIDBSubmitionException: test_row = TestResults(test_uid=results.test_uid, build_id=results.build_id, results=json_results) session.add(test_row) session.commit() Builds Services to handle build runs. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import builds from fastapi import FastAPI app = FastAPI() app.include_router(builds.router) run_builds(build_data, session, request) Submit a build request. The requests stores all the information requireed for the builds along with an identifier. Parameters: build_data ( BuildData ) \u2013 Data required by runners to perform a build session ( SessionDep ) \u2013 Database session request ( Request ) \u2013 Request object Source code in app/api/v1/endpoints/builds.py @router.post(\"/run\", status_code=204) def run_builds(build_data: BuildData, session: SessionDep, request: Request): \"\"\" Submit a build request. The requests stores all the information requireed for the builds along with an identifier. :param build_data: Data required by runners to perform a build :param session: Database session :param request: Request object \"\"\" build_runner = get_build_runner(build_data.runner) # Schedule the build build_uid = build_runner(build_data.toolchain, \"riscv\", build_data.tree, build_data.branch, build_data.kconfig, build_data.fragments, str(request.url_for(get_build_callback_funcname(build_data.runner)))) # Store it in the database scheduled_build = ScheduledBuild(build_uid=build_uid, toolchain=build_data.toolchain, tree=build_data.tree, branch=build_data.branch, kconfig=build_data.kconfig, fragments=build_data.fragments, runner=build_data.runner) session.add(scheduled_build) session.commit() Boot testing :: app.api.v1.endpoints.boot Sync Services that handle re-trying failed submsissions. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import sync from fastapi import FastAPI app = FastAPI() app.include_router(sync.router) sync_builds(session) async Looks for unsubmitted builds and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. Parameters: session ( SessionDep ) \u2013 Database session. Used to access stored results Source code in app/api/v1/endpoints/sync.py @router.post(\"/builds\", status_code=204) async def sync_builds(session: SessionDep): \"\"\" Looks for unsubmitted builds and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. :param session: Database session. Used to access stored results \"\"\" non_submitted_builds = session.exec(select(RunBuild).where(RunBuild.submitted == False)).all() for build in non_submitted_builds: build_uid = build.build_uid submission = build.submission # Only submit results with submitted false logging.info(f\"Submitting build for build uid {build_uid}\") try: submit_kcidb([submission]) mark_build_as_submitted(build_uid, session) except: logging.warning(f\"Could not submit build with uid {build_uid}\") session.commit() sync_results(session) async Looks for unsubmitted test results and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. Parameters: session ( SessionDep ) \u2013 Database session. Used to access stored results Source code in app/api/v1/endpoints/sync.py @router.post(\"/results\", status_code=204) async def sync_results(session: SessionDep): \"\"\" Looks for unsubmitted test results and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. :param session: Database session. Used to access stored results \"\"\" non_submitted_tests = session.exec(select(TestResults)).all() for test in non_submitted_tests: test_uid = test.test_uid results = test.results # Only submit results with submitted false logging.info(f\"Submitting results for test uid {test_uid}\") try: submit_kcidb(results) session.delete(test) mark_tests_as_submitted(test_uid, session) except: logging.warning(f\"Could not submit results for test uid {test_uid}\") session.commit() TuxSuite Callbacks TuxSuite runner callbacks. In their specific case they send a header that allows verifying the source of the request. Sadly, we have no way of using that header (at least for the moment), since we cannot obtain a signature in the community project. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import tuxsuite_callbacks from fastapi import FastAPI app = FastAPI() app.include_router(tuxsuite_callbacks.router) tuxsuite_boot_callback(x_tux_payload_signature, request, session) async Callback for tuxsuite boot test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteTestRequest ) \u2013 Test evaluation result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/boot\", status_code=204) async def tuxsuite_boot_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for tuxsuite boot test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Test evaluation result :param session: Database session \"\"\" # TODO: add payload signature check (when available) tests_results = request.status logging.info(f\"Received results for {tests_results.uid}\") test = session.exec(select(ScheduledTest).where(ScheduledTest.test_uid == tests_results.uid)).one() build_id = test.build_id # We mark the all tests from that test suit as received # TODO: Check how many of these are left as non-received and for how long submitted_tests = get_already_submitted_tests(build_id, tests_results.tests, session) parsed_test_results = await parse_tuxsuite_boot2kcidb(tests_results, test, submitted_tests) results = [item.to_json() for item in parsed_test_results] mark_as_received_tests_results([item.test for item in parsed_test_results], build_id, session) try: # Only submit results with submitted false submit_kcidb(results) mark_tests_as_submitted([item.test for item in parsed_test_results], build_id, session) except KCIDBSubmitionException: test_row = TestResults(test_uid=tests_results.uid, build_id=build_id ,results=results) session.add(test_row) session.commit() tuxsuite_build_callback(x_tux_payload_signature, request, session) async Callback for tuxsuite build. It obtains the build from the database, and marks its completion state. If it passed the build we submit it to KCIDB Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteBuildRequest ) \u2013 Build result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/build\", status_code=204) async def tuxsuite_build_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteBuildRequest, session: SessionDep): \"\"\" Callback for tuxsuite build. It obtains the build from the database, and marks its completion state. If it passed the build we submit it to KCIDB :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Build result :param session: Database session \"\"\" # TODO: add payload signature check build_results = request.status logging.info(f\"Received build results for {build_results.uid}\") try: build = session.exec(select(ScheduledBuild).where(ScheduledBuild.build_uid == build_results.uid)).one() except sqlalchemy.exc.NoResultFound: logging.warning(f\"Received unexpected build uid: {build_results.uid}\") raise HTTPException(status_code=500, detail=f\"Invalid build uid {build_results.uid}\") parsed_build_result = await parse_tuxsuite_build2kcidb(build_results, build) store_build_result(build_results, parsed_build_result, session) try: submit_kcidb([parsed_build_result.to_json()]) mark_build_as_submitted(build_uid=build_results.uid, session=session) except KCIDBSubmitionException: logging.warning(f\"Build {build_results.uid} couldn't be submitted\") tuxsuite_test_callback(x_tux_payload_signature, request, session) async Callback for tuxsuite test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteTestRequest ) \u2013 Test evaluation result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/test\", status_code=204) async def tuxsuite_test_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for tuxsuite test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Test evaluation result :param session: Database session \"\"\" # TODO: add payload signature check (when available) tests_results = request.status logging.info(f\"Received results for {tests_results.uid}\") test = session.exec(select(ScheduledTest).where(ScheduledTest.test_uid == tests_results.uid)).one() build_id = test.build_id # We mark the all tests from that test suit as received # TODO: Check how many of these are left as non-received and for how long submitted_tests = get_already_submitted_tests(build_id, tests_results.tests, session) parsed_test_results = await parse_tuxsuite_test2kcidb(tests_results, test, submitted_tests) results = [item.to_json() for item in parsed_test_results] mark_as_received_tests_results([item.test for item in parsed_test_results], build_id, session) try: # Only submit results with submitted false submit_kcidb(results) mark_tests_as_submitted([item.test for item in parsed_test_results], build_id, session) except KCIDBSubmitionException: test_row = TestResults(test_uid=tests_results.uid, build_id=build_id ,results=results) session.add(test_row) session.commit()","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#tests","text":"Services to handle tests runs and submissions. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import tests from fastapi import FastAPI app = FastAPI() app.include_router(tests.router)","title":"Tests"},{"location":"api/#app.api.v1.endpoints.tests.run_tests","text":"Schedules tests in a runner. We only scheduled tests that have not been run for before for the build. Parameters: tests_data ( TestSuite ) \u2013 Tests to schedule session ( SessionDep ) \u2013 Database session request ( Request ) \u2013 Request object, allows us to obtain a URL from a function name Source code in app/api/v1/endpoints/tests.py @router.post(\"/run\", status_code=204) async def run_tests(tests_data: TestSuite, session: SessionDep, request: Request): \"\"\" Schedules tests in a runner. We only scheduled tests that have not been run for before for the build. :param tests_data: Tests to schedule :param session: Database session :param request: Request object, allows us to obtain a URL from a function name \"\"\" tests_to_run = [] for test in tests_data.tests: tests_has_been_run = session.exec(select(func.count(RunTest.build_id)) .where(RunTest.build_id == tests_data.build_id) .where(RunTest.test == test)).one() if tests_has_been_run > 0: logging.info(f\"Test {test} from {tests_data.collection} has already been run for {tests_data.build_id}\") continue tests_to_run.append(test) if len(tests_to_run) == 0: logging.info(f\"No tests to run for build: {tests_data.build_id}\") return tests_runner = get_test_runner(tests_data.runner) test_uid = tests_runner(tests_data.kernel_image_url, tests_data.modules_url, tests_to_run, \"qemu-riscv64\", str(request.url_for(get_test_callback_funcname(tests_data.runner)))) scheduled_test = ScheduledTest(test_uid=test_uid, build_id=tests_data.build_id, test_collection=tests_data.collection, tests=tests_to_run, runner=tests_data.runner) session.add(scheduled_test) for test in tests_to_run: run_test = RunTest(build_id=tests_data.build_id, test=test) session.add(run_test) # Both queries are run in a transaction session.commit()","title":"run_tests"},{"location":"api/#app.api.v1.endpoints.tests.submit_results","text":"Submit test results to KCIDB. In this case test runner is not relevant. Parameters: results ( RunnerTestsResults ) \u2013 Test results passed as the body of the request session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tests.py @router.post(\"/submit\", status_code=204) async def submit_results(results: RunnerTestsResults, session: SessionDep): \"\"\" Submit test results to KCIDB. In this case test runner is not relevant. :param results: Test results passed as the body of the request :param session: Database session \"\"\" logging.info(f\"Received results for {results.test_uid}\") parsed_results = parse_results2kcidb(results) json_results = [item.to_json() for item in parsed_results] try: submit_kcidb(json_results) except KCIDBSubmitionException: test_row = TestResults(test_uid=results.test_uid, build_id=results.build_id, results=json_results) session.add(test_row) session.commit()","title":"submit_results"},{"location":"api/#builds","text":"Services to handle build runs. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import builds from fastapi import FastAPI app = FastAPI() app.include_router(builds.router)","title":"Builds"},{"location":"api/#app.api.v1.endpoints.builds.run_builds","text":"Submit a build request. The requests stores all the information requireed for the builds along with an identifier. Parameters: build_data ( BuildData ) \u2013 Data required by runners to perform a build session ( SessionDep ) \u2013 Database session request ( Request ) \u2013 Request object Source code in app/api/v1/endpoints/builds.py @router.post(\"/run\", status_code=204) def run_builds(build_data: BuildData, session: SessionDep, request: Request): \"\"\" Submit a build request. The requests stores all the information requireed for the builds along with an identifier. :param build_data: Data required by runners to perform a build :param session: Database session :param request: Request object \"\"\" build_runner = get_build_runner(build_data.runner) # Schedule the build build_uid = build_runner(build_data.toolchain, \"riscv\", build_data.tree, build_data.branch, build_data.kconfig, build_data.fragments, str(request.url_for(get_build_callback_funcname(build_data.runner)))) # Store it in the database scheduled_build = ScheduledBuild(build_uid=build_uid, toolchain=build_data.toolchain, tree=build_data.tree, branch=build_data.branch, kconfig=build_data.kconfig, fragments=build_data.fragments, runner=build_data.runner) session.add(scheduled_build) session.commit()","title":"run_builds"},{"location":"api/#boot-testing","text":":: app.api.v1.endpoints.boot","title":"Boot testing"},{"location":"api/#sync","text":"Services that handle re-trying failed submsissions. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import sync from fastapi import FastAPI app = FastAPI() app.include_router(sync.router)","title":"Sync"},{"location":"api/#app.api.v1.endpoints.sync.sync_builds","text":"Looks for unsubmitted builds and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. Parameters: session ( SessionDep ) \u2013 Database session. Used to access stored results Source code in app/api/v1/endpoints/sync.py @router.post(\"/builds\", status_code=204) async def sync_builds(session: SessionDep): \"\"\" Looks for unsubmitted builds and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. :param session: Database session. Used to access stored results \"\"\" non_submitted_builds = session.exec(select(RunBuild).where(RunBuild.submitted == False)).all() for build in non_submitted_builds: build_uid = build.build_uid submission = build.submission # Only submit results with submitted false logging.info(f\"Submitting build for build uid {build_uid}\") try: submit_kcidb([submission]) mark_build_as_submitted(build_uid, session) except: logging.warning(f\"Could not submit build with uid {build_uid}\") session.commit()","title":"sync_builds"},{"location":"api/#app.api.v1.endpoints.sync.sync_results","text":"Looks for unsubmitted test results and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. Parameters: session ( SessionDep ) \u2013 Database session. Used to access stored results Source code in app/api/v1/endpoints/sync.py @router.post(\"/results\", status_code=204) async def sync_results(session: SessionDep): \"\"\" Looks for unsubmitted test results and tries to submit them again. If the re try fails the results are kept stored, so that they can be submitted later. :param session: Database session. Used to access stored results \"\"\" non_submitted_tests = session.exec(select(TestResults)).all() for test in non_submitted_tests: test_uid = test.test_uid results = test.results # Only submit results with submitted false logging.info(f\"Submitting results for test uid {test_uid}\") try: submit_kcidb(results) session.delete(test) mark_tests_as_submitted(test_uid, session) except: logging.warning(f\"Could not submit results for test uid {test_uid}\") session.commit()","title":"sync_results"},{"location":"api/#tuxsuite-callbacks","text":"TuxSuite runner callbacks. In their specific case they send a header that allows verifying the source of the request. Sadly, we have no way of using that header (at least for the moment), since we cannot obtain a signature in the community project. They define their own router so they can be integrated into any app. from app.ap1.v1.endpoints import tuxsuite_callbacks from fastapi import FastAPI app = FastAPI() app.include_router(tuxsuite_callbacks.router)","title":"TuxSuite Callbacks"},{"location":"api/#app.api.v1.endpoints.tuxsuite_callbacks.tuxsuite_boot_callback","text":"Callback for tuxsuite boot test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteTestRequest ) \u2013 Test evaluation result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/boot\", status_code=204) async def tuxsuite_boot_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for tuxsuite boot test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Test evaluation result :param session: Database session \"\"\" # TODO: add payload signature check (when available) tests_results = request.status logging.info(f\"Received results for {tests_results.uid}\") test = session.exec(select(ScheduledTest).where(ScheduledTest.test_uid == tests_results.uid)).one() build_id = test.build_id # We mark the all tests from that test suit as received # TODO: Check how many of these are left as non-received and for how long submitted_tests = get_already_submitted_tests(build_id, tests_results.tests, session) parsed_test_results = await parse_tuxsuite_boot2kcidb(tests_results, test, submitted_tests) results = [item.to_json() for item in parsed_test_results] mark_as_received_tests_results([item.test for item in parsed_test_results], build_id, session) try: # Only submit results with submitted false submit_kcidb(results) mark_tests_as_submitted([item.test for item in parsed_test_results], build_id, session) except KCIDBSubmitionException: test_row = TestResults(test_uid=tests_results.uid, build_id=build_id ,results=results) session.add(test_row) session.commit()","title":"tuxsuite_boot_callback"},{"location":"api/#app.api.v1.endpoints.tuxsuite_callbacks.tuxsuite_build_callback","text":"Callback for tuxsuite build. It obtains the build from the database, and marks its completion state. If it passed the build we submit it to KCIDB Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteBuildRequest ) \u2013 Build result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/build\", status_code=204) async def tuxsuite_build_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteBuildRequest, session: SessionDep): \"\"\" Callback for tuxsuite build. It obtains the build from the database, and marks its completion state. If it passed the build we submit it to KCIDB :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Build result :param session: Database session \"\"\" # TODO: add payload signature check build_results = request.status logging.info(f\"Received build results for {build_results.uid}\") try: build = session.exec(select(ScheduledBuild).where(ScheduledBuild.build_uid == build_results.uid)).one() except sqlalchemy.exc.NoResultFound: logging.warning(f\"Received unexpected build uid: {build_results.uid}\") raise HTTPException(status_code=500, detail=f\"Invalid build uid {build_results.uid}\") parsed_build_result = await parse_tuxsuite_build2kcidb(build_results, build) store_build_result(build_results, parsed_build_result, session) try: submit_kcidb([parsed_build_result.to_json()]) mark_build_as_submitted(build_uid=build_results.uid, session=session) except KCIDBSubmitionException: logging.warning(f\"Build {build_results.uid} couldn't be submitted\")","title":"tuxsuite_build_callback"},{"location":"api/#app.api.v1.endpoints.tuxsuite_callbacks.tuxsuite_test_callback","text":"Callback for tuxsuite test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. Parameters: x_tux_payload_signature ( Annotated [ str | None, Header ()] ) \u2013 Payload signature used to verify the origin of the request request ( TuxSuiteTestRequest ) \u2013 Test evaluation result session ( SessionDep ) \u2013 Database session Source code in app/api/v1/endpoints/tuxsuite_callbacks.py @router.post(\"/test\", status_code=204) async def tuxsuite_test_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for tuxsuite test. It obtains the test from the database (to get its build id), stores the results in the TestResults table and marks the test as finished. :param x_tux_payload_signature: Payload signature used to verify the origin of the request :param request: Test evaluation result :param session: Database session \"\"\" # TODO: add payload signature check (when available) tests_results = request.status logging.info(f\"Received results for {tests_results.uid}\") test = session.exec(select(ScheduledTest).where(ScheduledTest.test_uid == tests_results.uid)).one() build_id = test.build_id # We mark the all tests from that test suit as received # TODO: Check how many of these are left as non-received and for how long submitted_tests = get_already_submitted_tests(build_id, tests_results.tests, session) parsed_test_results = await parse_tuxsuite_test2kcidb(tests_results, test, submitted_tests) results = [item.to_json() for item in parsed_test_results] mark_as_received_tests_results([item.test for item in parsed_test_results], build_id, session) try: # Only submit results with submitted false submit_kcidb(results) mark_tests_as_submitted([item.test for item in parsed_test_results], build_id, session) except KCIDBSubmitionException: test_row = TestResults(test_uid=tests_results.uid, build_id=build_id ,results=results) session.add(test_row) session.commit()","title":"tuxsuite_test_callback"},{"location":"configuration/","text":"Configuration Environment variables The app is configured via environment variables, which are defined in tow .env files. .env : Contains sereval definitions that affect the behaviour of the app itself. .env_caddy : Variables used to configure Caddy . These were splitted since they're used by two different containers. Templates for both of this files can be found in the root of the project, in .env.example and .env_caddy.example respectively. .env PROJECT_NAME : Name of the project BACKEND_CORS_ORIGINS : CORS configuration LOGS_FILE : Path to the file used to store logs DEBUG : true | false DB_URL : URL used to connect to the database DB_CERT_PATH : Path to the certificate used when performing an SSL conenction TUXSUITE_TOKEN : Access token used to connect to TuxSuite. Only required when using tuxsuite runner. Please contact the TuxSuite team to request a token KCIDB_SUBMIT_URL : KCIDB URL used to perform submissions KCIDB_TOKEN : Token required to perform submissions. Please contact the KernelCI team to get one .env_caddy EXT_ENDPOINT1 : URL used to access the VM LOCAL_1 : localhost LOCAL_2 : 127.0.0.1 , or any other IP used to refer to localhost Database The app requires a database to store tests that have already been run and build results. The connection to the database is setup using SQLAlchemy . You can find more information regarding SQLAlchemy database support here . The .env file contains to variables related to database configuration: DB_URL : Corresponds to the database URL. More information regarding how these should be constructed can be found in SQLAlchemy documentation . DB_CERT_PATH : Path to the certificate used when performing and SSL connection. If you're running the app in a container this file should be bind mounted , so that it becomes accessible inside the container. The app has been tested using SQLite and Postgres .","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#environment-variables","text":"The app is configured via environment variables, which are defined in tow .env files. .env : Contains sereval definitions that affect the behaviour of the app itself. .env_caddy : Variables used to configure Caddy . These were splitted since they're used by two different containers. Templates for both of this files can be found in the root of the project, in .env.example and .env_caddy.example respectively.","title":"Environment variables"},{"location":"configuration/#env","text":"PROJECT_NAME : Name of the project BACKEND_CORS_ORIGINS : CORS configuration LOGS_FILE : Path to the file used to store logs DEBUG : true | false DB_URL : URL used to connect to the database DB_CERT_PATH : Path to the certificate used when performing an SSL conenction TUXSUITE_TOKEN : Access token used to connect to TuxSuite. Only required when using tuxsuite runner. Please contact the TuxSuite team to request a token KCIDB_SUBMIT_URL : KCIDB URL used to perform submissions KCIDB_TOKEN : Token required to perform submissions. Please contact the KernelCI team to get one","title":".env"},{"location":"configuration/#env_caddy","text":"EXT_ENDPOINT1 : URL used to access the VM LOCAL_1 : localhost LOCAL_2 : 127.0.0.1 , or any other IP used to refer to localhost","title":".env_caddy"},{"location":"configuration/#database","text":"The app requires a database to store tests that have already been run and build results. The connection to the database is setup using SQLAlchemy . You can find more information regarding SQLAlchemy database support here . The .env file contains to variables related to database configuration: DB_URL : Corresponds to the database URL. More information regarding how these should be constructed can be found in SQLAlchemy documentation . DB_CERT_PATH : Path to the certificate used when performing and SSL connection. If you're running the app in a container this file should be bind mounted , so that it becomes accessible inside the container. The app has been tested using SQLite and Postgres .","title":"Database"},{"location":"dependencies/","text":"Project dependencies Services run in Docker containers, so most dependencies will be covered in the images. However, tu run the project locally you'll at least need: Python >= 3.12 docker docker compose make poetry >= 2.1.2 curl libpq-dev gcc autoconf automake build-essential libtool git libc-dev","title":"Dependencies"},{"location":"dependencies/#project-dependencies","text":"Services run in Docker containers, so most dependencies will be covered in the images. However, tu run the project locally you'll at least need: Python >= 3.12 docker docker compose make poetry >= 2.1.2 curl libpq-dev gcc autoconf automake build-essential libtool git libc-dev","title":"Project dependencies"},{"location":"deployment/","text":"Deployment Running the project locally The project provides a Makefile that allows running the app and installing dependencies. You can run the project locally in a container using make run-dev . This command will use docker-compose-dev.yml , which will build the app's container. If you want to run the app directly you can execute make run-app . Note that this will require you to install the dependencies beforehand, which can be done with make install . It is important to note that the make command is meant to be executed from the project's root directory. Deploy in production The project includes a release workflow that builds and publishes a docker image when creating a release. The docker-compose.yml uses that image to deploy a container with the app. You can use the image to deploy the app in any cloud service, or in any VM. Below we describe how the app can be deployed in any VM with systemd support, by running the containers as a service. 1. Install dependencies The VM will need docker , and docker compose installed. Details on how to do this can be found in dockerdocs . 2. Create necessary files in the VM Since this is considered a third-party app we recommend creating a folder in /opt/ . mkdir /opt/riscv-kci-bridge/ Then you'll need to get Caddyfile , docker-compose.yml , and log.ini files into this folder. You can get these files from the GitHub repo. /opt/riscv-kci-bridge/ caddy/ Caddyfile docker-compose.yml log.ini 3. Create a service file To enable the service you need to create a riscv-kci-bridge.service file in /etc/systemd/system . [Unit] Description=REST services to run tests and builds for RISC-V and submit results to KCIDB Requires=docker.service After=docker.service [Service] Restart=always ExecStart=docker compose -f /opt/riscv-kcidb-bridge/docker-compose.yml up ExecStop=docker compose -f /opt/riscv-kcidb-bridge/docker-compose.yml stop [Install] WantedBy=default.target 4. Enable the service To enabble the service run systemctl enable riscv-kci-bridge.service . 5. Start the service After enabling the service you can start using the service command. To start the service run service riscv-kci-bridge start . 6. Extra commands You can stop the service with service riscv-kci-bridge stop , and restart them with service riscv-kci-bridge restart . 7. Updating the app To update the app you need to: Poll the new version of the image: use docker pull image . Stop the containers: you can use service riscv-kci-bridge stop . Remove the existing container: you can use docker container ls --all to get all containers, and docker container rm container_id to remove the container. Start up the container again: with service riscv-kci-bridge start .","title":"Deployment"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"deployment/#running-the-project-locally","text":"The project provides a Makefile that allows running the app and installing dependencies. You can run the project locally in a container using make run-dev . This command will use docker-compose-dev.yml , which will build the app's container. If you want to run the app directly you can execute make run-app . Note that this will require you to install the dependencies beforehand, which can be done with make install . It is important to note that the make command is meant to be executed from the project's root directory.","title":"Running the project locally"},{"location":"deployment/#deploy-in-production","text":"The project includes a release workflow that builds and publishes a docker image when creating a release. The docker-compose.yml uses that image to deploy a container with the app. You can use the image to deploy the app in any cloud service, or in any VM. Below we describe how the app can be deployed in any VM with systemd support, by running the containers as a service.","title":"Deploy in production"},{"location":"deployment/#1-install-dependencies","text":"The VM will need docker , and docker compose installed. Details on how to do this can be found in dockerdocs .","title":"1. Install dependencies"},{"location":"deployment/#2-create-necessary-files-in-the-vm","text":"Since this is considered a third-party app we recommend creating a folder in /opt/ . mkdir /opt/riscv-kci-bridge/ Then you'll need to get Caddyfile , docker-compose.yml , and log.ini files into this folder. You can get these files from the GitHub repo. /opt/riscv-kci-bridge/ caddy/ Caddyfile docker-compose.yml log.ini","title":"2. Create necessary files in the VM"},{"location":"deployment/#3-create-a-service-file","text":"To enable the service you need to create a riscv-kci-bridge.service file in /etc/systemd/system . [Unit] Description=REST services to run tests and builds for RISC-V and submit results to KCIDB Requires=docker.service After=docker.service [Service] Restart=always ExecStart=docker compose -f /opt/riscv-kcidb-bridge/docker-compose.yml up ExecStop=docker compose -f /opt/riscv-kcidb-bridge/docker-compose.yml stop [Install] WantedBy=default.target","title":"3. Create a service file"},{"location":"deployment/#4-enable-the-service","text":"To enabble the service run systemctl enable riscv-kci-bridge.service .","title":"4. Enable the service"},{"location":"deployment/#5-start-the-service","text":"After enabling the service you can start using the service command. To start the service run service riscv-kci-bridge start .","title":"5. Start the service"},{"location":"deployment/#6-extra-commands","text":"You can stop the service with service riscv-kci-bridge stop , and restart them with service riscv-kci-bridge restart .","title":"6. Extra commands"},{"location":"deployment/#7-updating-the-app","text":"To update the app you need to: Poll the new version of the image: use docker pull image . Stop the containers: you can use service riscv-kci-bridge stop . Remove the existing container: you can use docker container ls --all to get all containers, and docker container rm container_id to remove the container. Start up the container again: with service riscv-kci-bridge start .","title":"7. Updating the app"},{"location":"runners/","text":"Runners We define a runner as a mechanism for running tests and/or builds. As shown in the diagram in the home, we assume the runners will work asynchronously. This means we will schedule builds, tests and boot tests and the runner will notify their completion. Furthermore, we assume that each runner accepts a URL as a callback, and that it performs a POST HTTP request when it finishes running. Depending on the use of the runner you'll need to either define a TestRunner , a BuildRunner or both. By default we have implemented a TuxSuite runner, which runs both tests and builds (note that TuxSuite offers 1,000 tests runs per month). Locating runners All available runners can be found in the runners.py file, under the AVAILABLE_RUNNERS type. This list is kept as an easy way to access the codes for all available runners. Defining new runners For each runner you need to define a code, a function to schedule tests or builds, and a set of callbacks. 1. Defining a code The code should be unique and be included in the AVAILABLE_RUNNERS in runners.py . For example, we can see that the TuxSuite runner has tuxsuite as a code. 2. Schedule tests and boot tests A test scheduler is any kind of callable that follows the TestRunner protocol . This can be used to schedule both tests and boot tests. This callable needs to send the test to the runner service, and return a unique identifier for the operation. This id will used when the callback is called to identify which operation was completed. Storing the identifier allows you to schedule multiple tests at once, even if the order in which they finished is different from how they were scheduled. The scheduler need to be defined in any module accesible by runners.py . For example, for the TuxSuite runner we define the test scheduler in tuxsuite_service.py . After creating it, the scheduler it needs to be added to the runners.py file, under the get_test_runner function. For example, let's assume we're adding a new demo runner. Integrating this runner could look like this: AVAILABLE_RUNNERS = Literal['tuxsuite', 'demo'] def get_test_runner(runner: str) -> TestRunner: match runner: case 'tuxsuite': return run_tuxsuite_tests # We add a new case for the demo runner case 'build': # demo_test_runner is implemented elsewhere and imported return demo_test_runner case _: raise RunnerNotSupported(runner) 3. Schedule builds A build scheduler is a callable that complies to the BuildRunner protocol . Build schedulers similarly to test schedulers, meaning they must return a unique identifier representing the build operation. For TuxSuite, the build runner can be found in tuxsuite_service.py . After creating it, the scheduler it needs to be added to the runners.py file, under the get_build_runner function. For example, adding a new demo build runner could look like this: AVAILABLE_RUNNERS = Literal['tuxsuite', 'demo'] def get_build_runner(runner: str) -> BuildRunner: match runner: case 'tuxsuite': return run_tuxsuite_build # We add a new case for the demo runner case 'demo': # demo_build_runner is implemented elsewhere and imported return demo_build_runner case _: raise RunnerNotSupported(runner) 4. Test, boot test and build callbacks Test and build callbacks are REST services that need to comply to the requirements of each runner. For instance, TuxSuite callbacks expect a x-tux-payload-signature header used to verify the origin of the request. There are several steps to defining the callbacks: a. Define the schemas The schemas represente the body of the requests. They are used by pydatinc to verify the body of the request is valid. Each runner might have a different schema. It is the responsability of the person adding the new runner to verify what the body of the callback request looks like. For example, for the tuxsuite runner we define the TuxSuiteTestRequest and the TuxSuiteBuildRequest models in the tuxsuite.py file. These follow the structure described in TuxSuite's documentation . b. Create the services A new file should be added to the backend/app/app/api/v1/endpoints folder. This file will contain the definition of all the necessary callbacks. For a runner with code demo the basic structure of the file might look as follows: from typing import Annotated from app.core.db import SessionDep from fastapi import APIRouter, Header from app.schemas.demo import DemoBuildRequest, DemoTestRequest # Create a fast api router for the services router = APIRouter() # Test callback, the status code we return depends on what the runner expects @router.post(\"/test\", status_code=204) async def demo_test_callback(x_demo_header: Annotated[str | None, Header()], request: DemoTestRequest, session: SessionDep): \"\"\" Callback for demo runner tests. Args: :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoTestRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... # Build callback, the status code we return depends on what the runner expects @router.post(\"/build\", status_code=204) async def demo_build_callback(x_demo_header: Annotated[str | None, Header()], request: DemoBuildRequest, session: SessionDep): \"\"\" Callback for demo runner builds. Args: :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoBuildRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... @router.post(\"/boot\", status_code=204) async def demo_boot_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for demo boot test. :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoTestRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... Please make sure that the names of the functions are unique inside the project. c. Add the services to the main router The services will only be visible after they're added to the main app router. To do so you need to modify the backend/app/app/api/v1/api.py file . For the demo runner it could look as follows: from fastapi import APIRouter from app.api.v1.endpoints import ( builds, tests, sync, tuxsuite_callbacks, demo ) api_router = APIRouter() # These are all the existing paths api_router.include_router(tuxsuite_callbacks.router, prefix=\"/tuxsuite/callback\", tags=[\"tuxsuite callbacks\"]) api_router.include_router(tests.router, prefix=\"/tests\", tags=[\"tests\"]) api_router.include_router(builds.router, prefix=\"/builds\", tags=[\"builds\"]) api_router.include_router(sync.router, prefix=\"/sync\", tags=[\"sync\"]) api_router.include_router(boot.router, prefix=\"/boot\", tags=[\"boot\"]) # We include the demo router here api_router.include_router(sync.router, prefix=\"/demo/callback\", tags=[\"demo callbacks\"]) After this the services under /demo/callback/test , /demo/callback/boot , and /demo/callback/build will be callable from the runner. 5. Configuring callback names By now you should now that TestRunner and BuildRunner have a callback name as a parameter. Instead of passing the strings directly we define a couple of functions that allow centralizing obtaining the names. Both functions are located in the runners.py file . To add a test callback name for a new demo runner you need to modify the get_test_callback_funcname function: def get_test_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_test_callback' case 'demo': # This is the name of the function we defined at test callback return 'demo_test_callback' case _: raise RunnerNotSupported(runner) For a build callback name for a new demo runner you need to modify the get_build_callback_funcname function: def get_build_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_build_callback' case 'demo': # This is the name of the function we defined at build callback return 'demo_build_callback' case _: raise RunnerNotSupported(runner) Finally, for a boot callback name for a new demo runner you need to modify the get_boot_callback_funcname function: def get_boot_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_boot_callback' case 'demo': # This is the name of the function we defined at boot callback return 'demo_boot_callback' case _: raise RunnerNotSupported(runner)","title":"Runners"},{"location":"runners/#runners","text":"We define a runner as a mechanism for running tests and/or builds. As shown in the diagram in the home, we assume the runners will work asynchronously. This means we will schedule builds, tests and boot tests and the runner will notify their completion. Furthermore, we assume that each runner accepts a URL as a callback, and that it performs a POST HTTP request when it finishes running. Depending on the use of the runner you'll need to either define a TestRunner , a BuildRunner or both. By default we have implemented a TuxSuite runner, which runs both tests and builds (note that TuxSuite offers 1,000 tests runs per month).","title":"Runners"},{"location":"runners/#locating-runners","text":"All available runners can be found in the runners.py file, under the AVAILABLE_RUNNERS type. This list is kept as an easy way to access the codes for all available runners.","title":"Locating runners"},{"location":"runners/#defining-new-runners","text":"For each runner you need to define a code, a function to schedule tests or builds, and a set of callbacks.","title":"Defining new runners"},{"location":"runners/#1-defining-a-code","text":"The code should be unique and be included in the AVAILABLE_RUNNERS in runners.py . For example, we can see that the TuxSuite runner has tuxsuite as a code.","title":"1. Defining a code"},{"location":"runners/#2-schedule-tests-and-boot-tests","text":"A test scheduler is any kind of callable that follows the TestRunner protocol . This can be used to schedule both tests and boot tests. This callable needs to send the test to the runner service, and return a unique identifier for the operation. This id will used when the callback is called to identify which operation was completed. Storing the identifier allows you to schedule multiple tests at once, even if the order in which they finished is different from how they were scheduled. The scheduler need to be defined in any module accesible by runners.py . For example, for the TuxSuite runner we define the test scheduler in tuxsuite_service.py . After creating it, the scheduler it needs to be added to the runners.py file, under the get_test_runner function. For example, let's assume we're adding a new demo runner. Integrating this runner could look like this: AVAILABLE_RUNNERS = Literal['tuxsuite', 'demo'] def get_test_runner(runner: str) -> TestRunner: match runner: case 'tuxsuite': return run_tuxsuite_tests # We add a new case for the demo runner case 'build': # demo_test_runner is implemented elsewhere and imported return demo_test_runner case _: raise RunnerNotSupported(runner)","title":"2. Schedule tests and boot tests"},{"location":"runners/#3-schedule-builds","text":"A build scheduler is a callable that complies to the BuildRunner protocol . Build schedulers similarly to test schedulers, meaning they must return a unique identifier representing the build operation. For TuxSuite, the build runner can be found in tuxsuite_service.py . After creating it, the scheduler it needs to be added to the runners.py file, under the get_build_runner function. For example, adding a new demo build runner could look like this: AVAILABLE_RUNNERS = Literal['tuxsuite', 'demo'] def get_build_runner(runner: str) -> BuildRunner: match runner: case 'tuxsuite': return run_tuxsuite_build # We add a new case for the demo runner case 'demo': # demo_build_runner is implemented elsewhere and imported return demo_build_runner case _: raise RunnerNotSupported(runner)","title":"3. Schedule builds"},{"location":"runners/#4-test-boot-test-and-build-callbacks","text":"Test and build callbacks are REST services that need to comply to the requirements of each runner. For instance, TuxSuite callbacks expect a x-tux-payload-signature header used to verify the origin of the request. There are several steps to defining the callbacks:","title":"4. Test, boot test and build callbacks"},{"location":"runners/#a-define-the-schemas","text":"The schemas represente the body of the requests. They are used by pydatinc to verify the body of the request is valid. Each runner might have a different schema. It is the responsability of the person adding the new runner to verify what the body of the callback request looks like. For example, for the tuxsuite runner we define the TuxSuiteTestRequest and the TuxSuiteBuildRequest models in the tuxsuite.py file. These follow the structure described in TuxSuite's documentation .","title":"a. Define the schemas"},{"location":"runners/#b-create-the-services","text":"A new file should be added to the backend/app/app/api/v1/endpoints folder. This file will contain the definition of all the necessary callbacks. For a runner with code demo the basic structure of the file might look as follows: from typing import Annotated from app.core.db import SessionDep from fastapi import APIRouter, Header from app.schemas.demo import DemoBuildRequest, DemoTestRequest # Create a fast api router for the services router = APIRouter() # Test callback, the status code we return depends on what the runner expects @router.post(\"/test\", status_code=204) async def demo_test_callback(x_demo_header: Annotated[str | None, Header()], request: DemoTestRequest, session: SessionDep): \"\"\" Callback for demo runner tests. Args: :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoTestRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... # Build callback, the status code we return depends on what the runner expects @router.post(\"/build\", status_code=204) async def demo_build_callback(x_demo_header: Annotated[str | None, Header()], request: DemoBuildRequest, session: SessionDep): \"\"\" Callback for demo runner builds. Args: :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoBuildRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... @router.post(\"/boot\", status_code=204) async def demo_boot_callback(x_tux_payload_signature: Annotated[str | None, Header()], request: TuxSuiteTestRequest, session: SessionDep): \"\"\" Callback for demo boot test. :x_demo_header (str): An example header. To obtain any other header from the request you just need to add an extra parameter to the function :request (DemoTestRequest): This is the model defined in the previous step. :session (SessionDep): Gives you access to the database \"\"\" ... Please make sure that the names of the functions are unique inside the project.","title":"b. Create the services"},{"location":"runners/#c-add-the-services-to-the-main-router","text":"The services will only be visible after they're added to the main app router. To do so you need to modify the backend/app/app/api/v1/api.py file . For the demo runner it could look as follows: from fastapi import APIRouter from app.api.v1.endpoints import ( builds, tests, sync, tuxsuite_callbacks, demo ) api_router = APIRouter() # These are all the existing paths api_router.include_router(tuxsuite_callbacks.router, prefix=\"/tuxsuite/callback\", tags=[\"tuxsuite callbacks\"]) api_router.include_router(tests.router, prefix=\"/tests\", tags=[\"tests\"]) api_router.include_router(builds.router, prefix=\"/builds\", tags=[\"builds\"]) api_router.include_router(sync.router, prefix=\"/sync\", tags=[\"sync\"]) api_router.include_router(boot.router, prefix=\"/boot\", tags=[\"boot\"]) # We include the demo router here api_router.include_router(sync.router, prefix=\"/demo/callback\", tags=[\"demo callbacks\"]) After this the services under /demo/callback/test , /demo/callback/boot , and /demo/callback/build will be callable from the runner.","title":"c. Add the services to the main router"},{"location":"runners/#5-configuring-callback-names","text":"By now you should now that TestRunner and BuildRunner have a callback name as a parameter. Instead of passing the strings directly we define a couple of functions that allow centralizing obtaining the names. Both functions are located in the runners.py file . To add a test callback name for a new demo runner you need to modify the get_test_callback_funcname function: def get_test_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_test_callback' case 'demo': # This is the name of the function we defined at test callback return 'demo_test_callback' case _: raise RunnerNotSupported(runner) For a build callback name for a new demo runner you need to modify the get_build_callback_funcname function: def get_build_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_build_callback' case 'demo': # This is the name of the function we defined at build callback return 'demo_build_callback' case _: raise RunnerNotSupported(runner) Finally, for a boot callback name for a new demo runner you need to modify the get_boot_callback_funcname function: def get_boot_callback_funcname(runner: str) -> str: match runner: case 'tuxsuite': return 'tuxsuite_boot_callback' case 'demo': # This is the name of the function we defined at boot callback return 'demo_boot_callback' case _: raise RunnerNotSupported(runner)","title":"5. Configuring callback names"}]}